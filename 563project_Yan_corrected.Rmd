---
title: "563project"
author: "Yan Liu"
date: "11/9/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

Read data and save a clean dataset after removing missing
```{r}
library(tidyverse)

data_whole<-read_csv("diabetes_corrected.csv")
#names(data_whole)

#visualize missing data
#data_whole[,2:8][data_whole[,2:8]==0]<- NA

#visualize missing data
#library(naniar)
#vis_miss(data_whole)

#give NAs to 0s which are impossible
#data_clean<-na.omit(data_whole)


#quantile(data_clean$Insulin, prob=c(.40,.5,.65))
#mean(data_clean$Insulin)
#sd(data_clean$Insulin)
```

Split data
Split the data into a training (70% of the data) and test set (30% of the data)
```{r}
library(caret)
library(rsample)
set.seed(14)
index <- initial_split(data_whole,
                       prop = 0.7)
train <- training(index)
test <- testing(index)
```

Data Plots
```{r}
library(corrplot)
res <- cor(data_whole)

#Plot the correlation matrix values by cluster
corrplot(res, type = "upper", order = "hclust",
         tl.col = "black", tl.cex = 0.5)

#table(data_clean$Outcome)
#mean(data_clean$Outcome)#NIR(66.8%), outcome has 262 "0" (66.8%) and 130 "1" (33.2%)

#table(data_clean$Outcome)/nrow(data_clean)
```

```{r,fig1, fig.height = 20, fig.width = 20}
library(PerformanceAnalytics)
chart.Correlation(data_whole, histogram = TRUE, method = "pearson")
```

change Outcome to factor
```{r}
train <- train %>% 
  mutate(Outcome = as.factor(Outcome))
test <- test %>% 
  mutate(Outcome = as.factor(Outcome))


#table(test$Outcome)

```

logistical regression. No clear collinearity according to VIF.
```{r}
glm<- glm(Outcome~.,train,family="binomial")
summary(glm)
par(mfrow=c(2,2))
plot(glm)
car::vif(glm)# a VIF value larger than 5 or 10 indicates a problematic amount of multicollinearity, No multicollinearity

# predicted probability of glm
glm_prob <- predict(glm, test,type = "response")
# predicted outcome of glm
glm_pred <- rep(0, length(glm_prob))
glm_pred[glm_prob > 0.5] <- 1
accuracy_glm<-mean(glm_pred == test$Outcome)

#glm_conf<-confusionMatrix(data =factor(glm_pred) , reference = test$Outcome)  
#glm_conf$overall[[1]]
#ROC and its AUC
library(pROC)
library(ggplot2)
roccurve_glm <- roc(response = test$Outcome, predictor = glm_prob)
ggroc(roccurve_glm, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc_glm<-auc(roccurve_glm)
```

backward selection
```{r}
back<-step(glm) #backward selection based on AIC 4 variables
back

# predicted probability of back
back_prob <- predict(back, test,type = "response")
# ROC and its auc
roccurve_back <- roc(response = test$Outcome, predictor = back_prob)
ggroc(roccurve_back, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc_back<-auc(roccurve_back)

# predicted outcome of glm
back_pred <- rep(0, length(glm_prob))
back_pred[back_prob > 0.5] <- 1
accuracy_back<-mean(back_pred == test$Outcome)

```

best selection
```{r}
train_back<-as.data.frame(model.matrix(Outcome~.-1, data = train))
library(bestglm)
best<-bestglm(train_back, IC="AIC")
best$BestModel

# predicted probability of glm
best_prob <- predict(best$BestModel, test,type = "response")
# ROC and its auc
roccurve_best <- roc(response = test$Outcome, predictor = best_prob)
ggroc(roccurve_best, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc_best<-auc(roccurve_best)

# predicted outcome of glm
best_pred <- rep(0, length(glm_prob))
best_pred[glm_prob > 0.5] <- 1
accuracy_best<-mean(best_pred == test$Outcome)

```

classification tree
```{r}
library(rpart)
library(rpart.plot)
tree_class <- rpart(
  Outcome ~ .,
  data = train,
  method = 'class',
  parms = list(split = "information"),
  control = rpart.control(
    xval = 10,
    minbucket = 2,
    cp = 0
  )
)
printcp(tree_class)
cp <- tree_class$cptable
tree_class_final <- prune(tree_class, cp = cp[3,1])#used minimum
rpart.plot(tree_class_final)

# test error rate using min cp
tree_pred <- predict(tree_class_final, newdata=test, type = "class")

accuracy_tree<-mean(tree_pred == test$Outcome)

tree_prob <- predict(tree_class_final, newdata=test, type = "prob")

roccurve_tree <- roc(response = test$Outcome, predictor = tree_prob[,2])
ggroc(roccurve_tree, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc_tree<-auc(roccurve_tree)

klaR::errormatrix(true = test$Outcome, predicted = tree_pred, relative = TRUE)
confusionMatrix(tree_pred,test$Outcome)

```

boost tree
```{r}
cvcontrol <- trainControl(method = "repeatedcv",
                          number = 5,
                          allowParallel = TRUE)
grid <- expand.grid(
  n.trees = c(10, 50, 100, 500, 1000),
  interaction.depth = c(1:3),
  shrinkage = c(0.01, 0.05, 0.1),
  n.minobsinnode = c(5,10)
)
capture <- capture.output(
  train.gbm <- train(
    Outcome ~ .,
    data = train,
    method = "gbm",
    trControl = cvcontrol,
    tuneGrid = grid
  )
)
train.gbm

boost_pred <- predict(train.gbm, newdata=test, type = "raw")
accuracy_boost<-mean(boost_pred == test$Outcome)


boost_prob <- predict(train.gbm, newdata=test, type = "prob")

roccurve_tree <- roc(response = test$Outcome, predictor = boost_prob[,2])
ggroc(roccurve_tree, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc_boost<-auc(roccurve_tree)

#klaR::errormatrix(true = test$Outcome, predicted = boost_pred, relative = TRUE)

```

random forests model
```{r}

library(randomForest)
randomF<-randomForest(Outcome~., data=train, mtry=3, importance=TRUE)
randomF

randomF_pred <- predict(randomF, newdata=test, type = "response")
accuracy_randomF<-mean(randomF_pred == test$Outcome)


randomF_prob <- predict(randomF, newdata=test, type = "prob")

roccurve_randomF <- roc(response = test$Outcome, predictor = randomF_prob[,2])
ggroc(roccurve_randomF, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc_randomF<-auc(roccurve_randomF)

varImpPlot(randomF)#Glucose is the most important one.
#klaR::errormatrix(true = test$Outcome, predicted = randomF_pred, relative = TRUE)
```

Knn
```{r}
## K values for tuning
kgrid <- expand.grid(k = seq(1, 51, by = 2))
## LOOCV tuning
tr <- trainControl(method = "repeatedcv",
                   number = 5,
                   repeats = 50)
## Train the classifier
knn <- train(
  Outcome ~ .,
  data = train,
  method = "knn",
  tuneGrid = kgrid,
  trControl = tr
)
plot(knn)
knn$bestTune$k

tuned_knn <- train(
  Outcome ~ .,
  data = train,
  method = "knn",
  tuneGrid = expand.grid(k = knn$bestTune$k),
  trControl = trainControl(method = "none")
)

knn_pred <- predict(tuned_knn,
                     newdata = test,
                     type = "raw")
accuracy_knn<-mean(knn_pred == test$Outcome)

knn_prob <- predict(tuned_knn,
                     newdata = test,
                     type = "prob")
roccurve_knn <- roc(response = test$Outcome, predictor = knn_prob[,2])
ggroc(roccurve_knn, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc_knn<-auc(roccurve_knn)
```

