---
title: "563project"
author: "Yan Liu"
date: "11/9/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

Read data
```{r}
library(tidyverse)

data_whole<-read_csv("diabetes.csv")
names(data_whole)

#data_whole%>%filter(data_whole$Glucose == 0)%>%nrow


#data_whole$Glucose[data_whole$Glucose == 0] <- NA
#data_whole$Insulin[data_whole$Insulin == 0] <- NA

data_whole[,2:8][data_whole[,2:8]==0]<- NA

sum(complete.cases(data_whole))
library(naniar)
vis_miss(data_whole)


data_clean<-na.omit(data_whole)

quantile(data_clean$Insulin, prob=c(.40,.5,.65))
mean(data_clean$Insulin)
sd(data_clean$Insulin)
```

Split data
Split the data into a training (70% of the data) and test set (30% of the data)
```{r}
library(caret)
library(rsample)
set.seed(14)
index <- initial_split(data_clean,
                       prop = 0.7)
train <- training(index)
test <- testing(index)
```

Data Plots

```{r}
library(corrplot)
res <- cor(data_clean)

#Plot the correlation matrix values by cluster
corrplot(res, type = "upper", order = "hclust",
         tl.col = "black", tl.cex = 0.5)

table(data_clean$Outcome)
mean(data_clean$Outcome)#NIR

table(data_clean$Outcome)/nrow(data_clean)
```


```{r,fig1, fig.height = 20, fig.width = 20}
library(PerformanceAnalytics)
chart.Correlation(data_whole, histogram = TRUE, method = "pearson")
```

```{r}
train <- train %>% 
  mutate(Outcome = as.factor(Outcome))
test <- test %>% 
  mutate(Outcome = as.factor(Outcome))


table(test$Outcome)
```

```{r}
glm<- glm(Outcome~.,train,family="binomial")
summary(glm)
par(mfrow=c(2,2))
plot(glm)
car::vif(glm)# a VIF value larger than 5 or 10 indicates a problematic amount of multicollinearity, No multicollinearity

glm_prob <- predict(glm, test,type = "response")
glm_pred <- rep(0, length(glm_prob))
glm_pred[glm_prob > 0.5] <- 1
error_glm<-mean(glm_pred != test$Outcome)

confusionMatrix(data = test$Outcome, reference = factor(glm_pred)) #?No Information Rate : 0.7749  

library(pROC)
library(ggplot2)
roccurve_glm <- roc(response = test$Outcome, predictor = glm_prob)
ggroc(roccurve_glm, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc(roccurve_glm)
#yhat_glm<-predict(glm,test)
#RMSE_glm<-sqrt(mean((test$shares - exp(yhat_lm))^2))
```

```{r}


```

```{r}
#backward selection after log transformation
library(leaps)
backward<- regsubsets(Outcome~., train, nvmax = 9, method = "backward")
backward_summary<-summary(backward)

#backward_summary[["which"]][size, ]
par(mfrow=c(1,3))
plot(backward_summary$cp, xlab = "Size", ylab = "backward Cp", type = "l")
plot(backward_summary$bic, xlab = "Size", ylab = "backward bic", type = "l")
plot(backward_summary$adjr2, xlab = "Size", ylab = "backward adjR2", type = "l")

coef(backward, which.min(backward_summary$cp))

#get best subset of the specified size with min cp.
# Create test model matrix, predcition, test error
sub <- backward_summary$which[which.min(backward_summary$cp), ]
test_model <- model.matrix(Outcome~ ., data = test)
test_back <- test_model[, sub]

back_odds<-exp(test_back %*% coef(backward, which.min(backward_summary$cp)))

back_prob<-back_odds/(1+back_odds)

roccurve_back <- roc(response = test$Outcome, predictor = back_prob)
ggroc(roccurve_back, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc(roccurve_back)

#back_pred <- rep(0, length(back_prob))
#back_pred[back_prob > 0.5] <- 1
#error_back<-mean(back_pred != test$Outcome)


```

```{r}
library(rpart)
library(rpart.plot)
tree_class <- rpart(
  Outcome ~ .,
  data = train,
  method = 'class',
  parms = list(split = "information"),
  control = rpart.control(
    xval = 10,
    minbucket = 2,
    cp = 0
  )
)
printcp(tree_class)
cp <- tree_class$cptable
tree_class_final <- prune(tree_class, cp = cp[4,1])#used minimum
rpart.plot(tree_class_final)

# test error rate using min cp
tree_pred <- predict(tree_class_final, newdata=test, type = "class")
tree_prob <- predict(tree_class_final, newdata=test, type = "prob")

roccurve_tree <- roc(response = test$Outcome, predictor = tree_prob[,2])
ggroc(roccurve_tree, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc(roccurve_tree)

klaR::errormatrix(true = test$Outcome, predicted = tree_pred, relative = TRUE)
```

```{r}
cvcontrol <- trainControl(method = "repeatedcv",
                          number = 5,
                          allowParallel = TRUE)
grid <- expand.grid(
  n.trees = c(10, 50, 100, 500, 1000),
  interaction.depth = c(1:3),
  shrinkage = c(0.01, 0.05, 0.1),
  n.minobsinnode = c(5,10)
)
capture <- capture.output(
  train.gbm <- train(
    Outcome ~ .,
    data = train,
    method = "gbm",
    trControl = cvcontrol,
    tuneGrid = grid
  )
)
train.gbm

boost_pred <- predict(train.gbm, newdata=test, type = "raw")
boost_prob <- predict(train.gbm, newdata=test, type = "prob")

roccurve_tree <- roc(response = test$Outcome, predictor = boost_prob[,2])
ggroc(roccurve_tree, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc(roccurve_tree)

klaR::errormatrix(true = test$Outcome, predicted = boost_pred, relative = TRUE)

```


```{r}
#random forests model
library(randomForest)
randomF<-randomForest(Outcome~., data=train, mtry=3, importance=TRUE)
randomF

randomF_pred <- predict(randomF, newdata=test, type = "response")
randomF_prob <- predict(randomF, newdata=test, type = "prob")

roccurve_randomF <- roc(response = test$Outcome, predictor = randomF_prob[,2])
ggroc(roccurve_randomF, legacy.axes = TRUE, lwd=2) +theme_bw(base_size = 18)
auc(roccurve_randomF)

varImpPlot(randomF)
klaR::errormatrix(true = test$Outcome, predicted = randomF_pred, relative = TRUE)
```
